{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7308aef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import category_encoders as ce\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import r2_score\n",
    "from joblib import parallel_backend\n",
    "from ray.util.joblib import register_ray\n",
    "register_ray()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2b93e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('HousePrices.csv')\n",
    "df = df.drop('Amenities',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ea2c870",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Price',axis=1)\n",
    "y = df['Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc9d6606",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AreaTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    def transform(self,X,y=None):\n",
    "        X['Area'] = pd.to_numeric(X['Area'].str.replace('[^.0-9]', ''))\n",
    "        return X\n",
    "class BHKTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    def transform(self,X,y=None):\n",
    "        X['BHK'] = pd.to_numeric(X['BHK'].str.replace('[^.0-9]', ''))\n",
    "        return X\n",
    "class LocationTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    def transform(self,X,y=None):\n",
    "        Location_stats = X['Location'].value_counts(ascending=False)\n",
    "        Location_stats_less_than_10 = Location_stats[Location_stats<=10]\n",
    "        X.Location = X.Location.apply(lambda x : 'other' if x in Location_stats_less_than_10 else x)\n",
    "        dummies = pd.get_dummies(X.Location)\n",
    "        X = pd.concat([X.drop('Location',axis='columns'),dummies.drop('other',axis='columns')],axis='columns')\n",
    "        return X\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "X_CustomPipeline = Pipeline(steps=[\n",
    "    ('AreaTransformer',AreaTransformer()),\n",
    "    ('BHKTransformer',BHKTransformer())\n",
    "])\n",
    "X_CustomPipelineLocation = Pipeline(steps=[\n",
    "    ('LocationTransformer',LocationTransformer()\n",
    "     )])\n",
    "X_NumericPipeline = Pipeline(steps=[\n",
    "    ('Simple Imputer',SimpleImputer(strategy='median')),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8aa2cb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_price(value):\n",
    "    if 'Cr' in value:\n",
    "        new_value1 = re.sub(re.compile('[^.0-9]'), '', value)\n",
    "        return float(new_value1)*100\n",
    "    elif 'Lac' in value:\n",
    "        new_value2 = re.sub(re.compile('[^.0-9]'), '', value)\n",
    "        return float(new_value2)\n",
    "    else:\n",
    "        return None\n",
    "class PriceTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    def transform(self,X,y=None):\n",
    "        X = X.apply(convert_price)\n",
    "        X = np.array(X)\n",
    "        X = X.reshape(-1,1)\n",
    "        return X\n",
    "class PriceLogTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    def transform(self,X,y=None):\n",
    "        X = X.ravel()\n",
    "        return np.log(X)\n",
    "y_CustomPipeline = Pipeline(steps=[\n",
    "    ('Price Transformer',PriceTransformer())\n",
    "])\n",
    "y_NumericPipeline = Pipeline(steps=[\n",
    "    ('Simple Imputer',SimpleImputer(strategy='median')),\n",
    "    ('Log Transformer',PriceLogTransformer())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51581165",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_CustomPipelineLocation.transform(X)\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "X_train = X_CustomPipeline.transform(X_train)\n",
    "X_train_final = X_NumericPipeline.fit_transform(X_train[['Area','BHK']])\n",
    "X_train['Area'] = X_train_final[:,0]\n",
    "X_train['BHK'] = X_train_final[:,1]\n",
    "\n",
    "X_test = X_CustomPipeline.transform(X_test)\n",
    "X_test_final = X_NumericPipeline.fit_transform(X_test[['Area','BHK']])\n",
    "X_test['Area'] = X_test_final[:,0]\n",
    "X_test['BHK'] = X_test_final[:,1]\n",
    "\n",
    "y_train = y_CustomPipeline.fit_transform(y_train)\n",
    "y_train = y_NumericPipeline.fit_transform(y_train)\n",
    "\n",
    "y_test = y_CustomPipeline.fit_transform(y_test)\n",
    "y_test = y_NumericPipeline.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de36c02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PoolActor pid=11680)\u001b[0m C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\u001b[32m [repeated 5x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(PoolActor pid=11680)\u001b[0m If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PoolActor pid=11680)\u001b[0m \u001b[32m [repeated 30x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PoolActor pid=11680)\u001b[0m from sklearn.pipeline import make_pipeline\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PoolActor pid=11680)\u001b[0m model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PoolActor pid=11680)\u001b[0m If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PoolActor pid=11680)\u001b[0m kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PoolActor pid=11680)\u001b[0m model.fit(X, y, **kwargs)\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PoolActor pid=11680)\u001b[0m   warnings.warn(\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PoolActor pid=11680)\u001b[0m C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PoolActor pid=11896)\u001b[0m C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:397: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
      "\u001b[2m\u001b[36m(PoolActor pid=11896)\u001b[0m C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:397: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
      "\u001b[2m\u001b[36m(PoolActor pid=9900)\u001b[0m C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PoolActor pid=9900)\u001b[0m If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PoolActor pid=9900)\u001b[0m \u001b[32m [repeated 24x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PoolActor pid=9900)\u001b[0m from sklearn.pipeline import make_pipeline\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PoolActor pid=9900)\u001b[0m model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PoolActor pid=9900)\u001b[0m If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PoolActor pid=9900)\u001b[0m kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PoolActor pid=9900)\u001b[0m model.fit(X, y, **kwargs)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PoolActor pid=11896)\u001b[0m   warnings.warn(\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PoolActor pid=9900)\u001b[0m C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               model  best_score  \\\n",
      "0  linear_regression    0.700849   \n",
      "1              lasso    0.306482   \n",
      "2      decision_tree    0.745617   \n",
      "3   gradient_booster    0.823123   \n",
      "4        ada_booster    0.790997   \n",
      "\n",
      "                                         best_params  \n",
      "0                               {'normalize': False}  \n",
      "1                {'alpha': 1, 'selection': 'random'}  \n",
      "2         {'criterion': 'mse', 'splitter': 'random'}  \n",
      "3  {'criterion': 'friedman_mse', 'learning_rate':...  \n",
      "4  {'learning_rate': 0.1, 'loss': 'exponential', ...  \n"
     ]
    }
   ],
   "source": [
    "def find_best_model_using_gridsearchcv(X,y):\n",
    "    algos = {\n",
    "        'linear_regression' : {\n",
    "            'model': LinearRegression(),\n",
    "            'params': {\n",
    "                'normalize': [True, False]\n",
    "            }\n",
    "        },\n",
    "        'lasso': {\n",
    "            'model': Lasso(),\n",
    "            'params': {\n",
    "                'alpha': [1,2],\n",
    "                'selection': ['random', 'cyclic']\n",
    "            }\n",
    "        },\n",
    "        'decision_tree': {\n",
    "            'model': DecisionTreeRegressor(),\n",
    "            'params': {\n",
    "                'criterion' : ['mse','friedman_mse','squared_error'],\n",
    "                'splitter': ['best','random']\n",
    "            }\n",
    "        },\n",
    "        'gradient_booster': {\n",
    "            'model': GradientBoostingRegressor(),\n",
    "            'params': {\n",
    "                'loss': ['squared_error', 'absolute_error'],\n",
    "                'learning_rate': [0.1,1,1.5,2],\n",
    "                'n_estimators': [10,50,100,150,200],\n",
    "                'criterion': ['friedman_mse', 'squared_error']\n",
    "            }\n",
    "        },\n",
    "        'ada_booster': {\n",
    "            'model': AdaBoostRegressor(),\n",
    "            'params': {\n",
    "                'n_estimators': [10,50,100,150,200],\n",
    "                'learning_rate': [0.1,1,1.5,2],\n",
    "                'loss': ['linear','square','exponential']\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    scores = []\n",
    "    cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "    for algo_name, config in algos.items():\n",
    "        gs =  GridSearchCV(config['model'], config['params'], cv=cv, return_train_score=False)\n",
    "        with parallel_backend('ray'): # We are using multithreading to speed up the process of training our models.\n",
    "            gs.fit(X,y)\n",
    "        scores.append({\n",
    "            'model': algo_name,\n",
    "            'best_score': gs.best_score_,\n",
    "            'best_params': gs.best_params_\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(scores,columns=['model','best_score','best_params'])\n",
    "\n",
    "best_model = find_best_model_using_gridsearchcv(X_train,y_train)\n",
    "print(best_model.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67187da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score: 0.8189436946366715\n"
     ]
    }
   ],
   "source": [
    "params = dict(best_model.iloc[3]['best_params'])\n",
    "model = GradientBoostingRegressor(**params)\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(f'R2 Score: {r2_score(y_test,y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf6065a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
